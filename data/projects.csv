1,Font Color Predictor,09/28/18,https://github.com/brian-w-projects/Text-Background-Color-Learner ,1,Using Machine learning to Determine Preferred Text Colors,This project uses machine learning to train a model to predict which of two potential text colors is better suited for reading against any color background. Each user answers 100 “this or that” questions displayed in a Tkinter window and this information is used to predict future responses. Font colors may be selected or chosen randomly.,"In designing this project, I made a number of conscious decisions to keep it as streamlined as possible. Instead of looking for a comprehensive solution to the task, I instead focused on how much of the task was achievable with a minimal amount of code. For example, the program generates the background colors randomly instead of systematically presenting a representative sample of the overall color palette. In fact, it is theoretically possible for the same color to be displayed twice (and for the user to pick a different answer each time). Possible extensions to this project include addressing this issue and allowing for multi-classification instead of only two colors. The algorithm is often able to achieve over 90% accuracy given consistent user feedback.",D3QkF0KDC2A
2,Hockey Playoff Predictor,10/01/18,https://github.com/brian-w-projects/NHL-Playoff-Predictor ,1,Using Machine Learning to Predict the NHL Playoff Teams,"This project is designed to predict which teams will make the NHL playoffs after half the games of the season have been played. A machine learning model drew on the historical data of a small number of team statistics. In January 2019, I made predictions for the current NHL season. Validation data suggested that I would correctly identify 28 or 29 of the 31 teams. In April, I compared my predictions to the correct answers and my model had made 28 correct predictions.","The workflow on this project is purposefully not as streamlined as I would normally code for a development project. Instead of simply scraping the exact statistics I needed, I decided to scrap the individual raw game data and perform the calculations myself. This required me to store the data in MySQL and then write queries to gather the information needed to feed the machine learning model. Thus, the data was scraped with Python, passed in MySQL for querying before being loaded back into Python to build the machine learning model.",ykzbCkNoTSo
3,Blockbuster Movie Boxoffice,06/01/18,https://brian-cuny.shinyapps.io/App-2/ ,1,Using R and Shiny to Scrape and Analyze Recent Movie Blockbusters,"This data analysis project was written using R and served as the final project for my ‘Data Acquisition and Management’ course. Scraping data from multiple sources, I compiled information on the top 50 blockbuster movies for each of the past 10 years including their box-office totals, genres, headlining stars and their respective genders. I used this data to create a Shiny app to explore trends in gender diversity across the data.","The data for this project was pulled from several sources. I used traditional scraping to gather the names of popular movies and their box office totals. I then used an API to query additional information from IMDB. Finally, I scraped information from Wikipedia to gather details on the gender of the leads. All of this was cleaned and combined into the final project. Identifying movie star’s genders was the most challenging task as there is no simple resource that contains this information. I decided to use Wikipedia, as their standardized format for writing articles made it possible to scrape the first paragraph and analyze the text for gendered pronouns. This pipeline efficiently and effectively gathered the data for further analysis. ",5L5h_gu_ec8
4,My Personal Website,10/04/18,https://github.com/brian-w-projects/personal_website,1,Portfolio Website Designed with Flask/MySQL and Deployed on Heroku,"I developed my personal website entirely from scratch to house my portfolio and personal projects. I utilized HTML, CSS, Bootstrap and some basic jQuery on the front end and Flask and SQL on the back end. The site is currently being hosted on Heroku.","I have found the ability to quickly make small web apps to be a useful skill. There are often times when I want to quickly prototype an idea for a new project and will create small pages to test out these ideas. In addition, making websites has helped me to learn how to create and interact with APIs. It also provides me with practice designing SQL schema and querying the underlying data. Even presentations in Rmarkdown can be styled with Bootstrap. Being comfortable with these technologies can simplify various data science tasks and has proved useful on numerous occasions.",none
5,Java API Example,05/30/17,https://github.com/brian-w-projects/access_recommend_me/tree/master/src/recommendation,1,Java Program to Interact with API,"Several years ago when I was first learning Flask, I developed a Twitter clone website site that I called “Recommend Me”. This project allowed me to focus on the front and back end of web development by recreating a popular website. In addition to being my first experience with web technologies, it was also my first time creating an API which I used to efficiently interact with the site. I decided to test the design of the API by creating this Java project.","This Java project allows for complete interaction with my Twitter clone website via its public facing API. Every action on the site can be completed either via the command line or with a text file of commands. The commands also feature a number of convenience wrappers for accessing larger amounts of information with few commands. <br/><br/> Although the program itself is usable only with the “Recommend Me” site, I believe it demonstrates benefit of having knowledge in a variety of programming languages. My knowledge in Java enabled me to create this program to aid in the development of the “Recommend Me” project. I believe it is important to have comfort in both higher level and lower level languages as both have their places depending on the core task at hand. This is part of the reason I decided to become an Oracle Certified Professional in Java.",none
6,Health Tweets Data Exploration,10/10/18,https://github.com/brian-w-projects/Health-Tweet-Analysis ,0,Data Exploration and Analysis Presentation on Health News Service Tweets,"Data sets often hold many important insights that are not readily discernible. For this project, I took a deep dive into a publicly available data set that contained thousands of Tweets sent by a variety health news organizations. I set out with no singular goal in mind, instead using my data exploration and analysis skills to discover whatever I could about the data set. I let the information I discovered lead me to the next discovery until I could create a cohesive image of the dataset as a whole. ","This project demonstrates the importance of being well versed in a variety of technologies. I utilized R’s Tidytext tools to perform a deep analysis on the variety of tones and unique wordings each organization used. Using R allowed me to quickly and easily use Tidyverse for cleaning and plotting the data. From there, I used Python to perform unsupervised machine learning to help categorize the topics discussed in the data set. I was able to identify 20 clear topics and voices in the dataset. <br/><br/> One of the major benefits of being comfortable using both R and Python is that it gives me access to a variety of tools so that I can select the one best suited for the designated task. All of this information was combined into a report and presentation which you can view below.",sV1AMqjZTTw
7,USA Metro Area Population Visualizations,11/29/18,https://github.com/brian-w-projects/US-Metro-Data-With-Plotly ,1,US City Population Changes Visualized with Plotly and Deployed with Flask/MySQL,"This data visualization project was created using MySQL, Python, Flask, HTML, jQuery and most importantly Plotly. It was the final project for my ‘Data Visualizations’ course. I ultimately created a single page web app that allowed users to explore changes in the population of US metro areas over the past decade. Great care was taken to push as much of the workload as possible to the end user alleviating bottle-necking on my server.","The project consisted of three separate phases, each highlighting a different area of the data science pipeline. <br/><br/> First, I needed to scrape and clean the required data sets. The data was supplied by a variety of US government websites and consisted of the 300+ official US metro area designations, their associated counties and populations from 2010 to 2017. The data was munged and placed into a MySQL database for easy querying. <br/><br/> Second, I created a Flask app to serve the content. While Plotly can be coded in a variety of languages, I wanted the back end to be streamlined in order to serve as many concurrent users as possible. As a result, the app itself has very little actual content outside of accepting requests and serving the associated data from the database. <br/><br/> Finally, the bulk of the project consisted of the single web page that allows users to request information on metro areas and serves them with a series of interactive displays to aid in examining the data. It is my hope that using this page will allow analysis of populations trends in the United States.",KDrHl8qwD24
8,Hockey Playoff Predictor – Distributed,01/16/19,https://github.com/brian-w-projects/NHL-Playoff-Predictor ,1,Simulating Big Data Processing and Machine Learning on Google Cloud Platform,"This project is an expansion of the previously created ‘Hockey Playoff Predictor’ project. In order to solidify my understanding of Google Cloud Platform (GCP), I migrated the previous project to the cloud and refactored all of my code to function in a distributed fashion. I utilized a number of GCP services including Dataproc and Big Query. The pipeline I created involved scraping the raw data from the Internet, extracting the necessary information and saving it to Big Query. From there the data was further processed until it could be loaded into Spark to create a machine learning model. ","Throughout the process, I simulated working with big data even though the dataset was actually small enough to be worked on locally. This required that I stored all of my data in the cloud and utilized distributed computing to complete my exploration of the data and creation of the machine learning model. This allowed me to gain experience working with big data while keeping my costs for using GCP manageable. Comparing my locally created model to the newly developed SparkML model, I found their predictions to be identical.",HBgS6G8JHFc
9,Pokemon Pokedex Generator,04/01/19,https://github.com/brian-w-projects/Pokedex-Generator ,0,Using a Neural Network to Create Pokedex Entries,"As a fun side project, I created a small neural network and trained it on all the existing Pokedex entries from the Pokemon series of video games. The trained network can generate new Pokedex entries by rearranging and connecting common words and phrases from existing entries.","I began by scraping from the internet all the existing Pokedex entries, tokenizing the results and splitting the results into trigrams. I created a recurrent neural network based on an LSTM that took two words as input and learned the third word. This project is significantly different than a traditional deep learning project in that it begins as a supervised learning problem but results in a model that develops entirely new results. In order to achieve this, a few modifications needed to be made. <br/><br/> First, I did not want the model to have too much predictive ability. If the model could predict the “correct” next word too accurately then I would simply end up recreating existing Pokedex entries. I needed to find a balance that allowed for variation in the responses. I can further modulate this randomness with a variable that alters the output distribution of word selection and then selects the next word with a random weighted selection. Feeding this result back into the net over and over creates a variety of Pokedex entries based on one or two seed words. I found this project to be a fun and enjoyable as a side project.",none
10,CitiBike Dashboard – CapStone Project,04/24/19,https://github.com/brian-w-projects/Citibike-Capstone-Project ,0,Predicting and Displaying Bike Share Usage Rates with Machine Learning,"An excerpt from my final paper is below. <br/><br/> Over the past 20 years bike share systems have exploded in popularity in major cities across North America, Europe and Asia. It is estimated that there are more than 700 cities operating over 800,000 bikes in 50 countries around the world. As these programs grow so does their need for up to the minute information on bike operations and strong models that can predict usage rates. <br/><br/> Currently, the most popular bike share systems utilize automated stations. These semi-permanent structures house bikes built and designed specifically for use with the system. Riders use their smartphones to unlock a bike at any station and are permitted to ride it anywhere in the designated operating area. Once the ride is finished, they can dock the bike at any station that has an empty space. They do not need to dock at the station where the bike was initially rented. The rider is automatically charged based on the amount of time the bike was in use. Most bike share companies offer a variety of different rental price points to appeal to everyone from tourists to daily commuters. For the purpose of my capstone project, I will imagine a scenario where I have been hired as a consultant by the Citi Bike bike share program which currently has exclusive rights to operate in New York City and Jersey City, New Jersey. <br/><br/> Although the specially built bikes have been designed to ensure stable long term performance, there is a need for employees to be on the ground every day. Broken bikes will need to be fixed or removed from circulation, tires will need to be inflated and, most commonly, bikes will need to be moved from full stations to empty stations to avoid clumping. The company wants to avoid a situation where a person cannot rent a bike because a station is empty or cannot park a bike because a station is full. <br/><br/> The employees who perform this work are often independent contractors who are paid only for the hours they work. As the number of rides taken on any given day can vary widely, it is important for Citi Bike to have an accurate estimate as to the number of rides on a given day and hour. This will allow them to call in enough independent contractors to fulfill the needs for the day without paying workers who are not needed. <br/><br/> Citi Bike requires hourly predictions of the number of bikes that will be rented 24 hours in advance of the beginning of each day. This will provide them sufficient time to call in the appropriate number of workers. For example, at 1 AM on May 5th, 2019, I will need to provide 24 predictions, one for each hour, for the number of bikes that will be rented during the entirety of May 6th, 2019.","This comprehensive project is the result of many months worth of work. The paper I wrote based on my research can be found in my github repo while a presentation on the project can be found below. This project utilized the following technologies: <ul><li>Python was used to scrape the raw data from the internet and its associated data science libraries (incl. Jupyter, Pandas, Scikit Learn and Keras) were used to perform EDA and to create the predictive models.</li> <li>R (tidyverse) was used to create the charts and maps used in this paper.</li> <li>Flask created the small apps that were used to schedule recurring jobs.</li> <li>Java was used to create an ETL pipeline via Apache Beam.</li> <li>Apache Spark was used to create the graph network via PySpark.</li> <li>Google Cloud Platform was used extensively to aid in every step of the project.</li></ul>",9tvlh18F-Rw
11,Kaggle Competition Presentation,05/30/19,none,0,Live Presentation Given to Vancouver Data Science Meetup Community,I am a member of the Vancouver Data Science Meetup community. This amazing community offers a number of group meetings focused on learning data science. The group holds a bi-weekly meeting where a member presents on a recently held Kaggle competition with an emphasis on what can be learned from the high ranking entries. The structure of the meetup allows for an interactive presentation with plenty of time for questions and answers.,"The below youtube video is my presentation to the meetup group. It serves to highlight my ability to construct a long form presentation, with question and answer session, and present it publicly. The presentation is designed to be accessible to beginning data scientists and acts primarily as an instructive tool to aid future research.",ageh45rxyXU
12,Sodoku Image Recognition,05/13/19,https://github.com/brian-w-projects/Sodoku-Image-Recognition ,0,Using an Object Detection Neural Network to Solve Pictures of Sodoku Puzzles,"This project trains a Faster R-CNN based neural network to identify the digits in an unsolved Sodoku puzzle along with their locations. Using this information, I created a script that collects the output and solves the Sodoku puzzle, returning an image of the solved puzzle as a final result. ","This project consisted of several steps. I began by taking pictures of hundreds of Sodoku puzzles from a variety of angles. I then meticulously drew bounding boxes on each image. I created an object detecting network by using the collected data and leveraging transferred learning by adapting a pre-trained image classification network. The output of my model has boxes drawn around each number in the puzzle. From there, I wrote a script that interprets this output to recreate the original puzzle and a second one that solves the puzzle and displays the result. <br/><br/> The biggest limitation on this project is the amount of time it took to collect the training data. With a larger set of data, I could create a program that does not require the puzzle to be cropped or that could solve partially completed puzzles (with handwritten digits).",none
13,Java File Cleaner,12/07/18,https://github.com/brian-w-projects/Filename-Cleaner ,1,Java Program to Clean and Organize Filenames,This small program quickly renames all the files in a folder to match the name of the folder. I had originally created this program to help me organize all of the images stored on my computer. It makes use of new Java 8 features in order to maximize speed and efficiency.,"While writing this project I never anticipated that I would have the opportunity to use it in one of my portfolio projects. However, as part of the process of training the Sodoku solving neural network, I needed to take hundreds of pictures of Sodoku puzzles. The process of collecting and labeling the images was arduous and I wanted to make sure I didn’t accidentally leak data. I made frequent use of this small program to keep my images organized.",none
14,Python File Cleaner,07/19/19,https://github.com/brian-w-projects/Picture-Organizer ,1,Python Program to Clean and Organize Files,This small program quickly renames all the files inside of a folder to match the containing folder. This is incredibly helpful for organizing photos in time series order with a simple naming scheme.,"This project offers a small number of options for customization that include skipping inner folders or skipping files with certain extensions. This allows, for example, the renaming of all photos but not videos. In addition to the code there is a complete testing suite to ensure that additional updates do not break functionality. This project is fun to return to as there are always additional features that can be added. This makes the testing suite all the more important.",none
15,Twitter Sentiment Predictor,08/06/19,https://github.com/brian-w-projects/Twitter-Sentiment ,0,Twitter Sentiment Prediction with Neural Network,"This project is a Keras trained neural network wrapped in a single page flask app packaged with docker and deployed on GCP's Kubernetes Engine. Users enter a search term and tweets containing these terms are scraped with Twitter's public API and passed through my trained neural network to determine their sentiment. The results are displayed for the user and allow the tracking of sentiment for people, places or companies in near real time. <br/><br/> This neural network can be used by brands to monitor potentially flattering or problematic memetic trends on Twitter before they go viral, allowing the brand to interject themselves appropriately.","This project utilizes a number of technologies that are integral to quickly developing, testing and deploying a working model. I believe it is critical to be fully versed in each of these technologies. After all, there is no benefit to creating something that cannot be deployed and used by the public. <br/><br/> The first step in this project was to train a neural network to determine whether the content of a tweet is positive or not. This was done using Keras to train a bidirectional lstm based network that received GLOVE embedded tokenized words as it’s input. The output is a simple probability on the confidence of positivity. This model was saved and wrapped in a simple single page Flask app. The app passes along the entered search terms to Twitter’s public facing API to scrape related tweets. The tweets are then passed through my neural network and the results are displayed to the user. <br/><br/> The app was uploaded to docker hub for easy deployment into a Kubernetes cluster. Kubernetes is the perfect framework to deploy this app so that multiple containers can be deployed and scaled as the popularity of the tool grows or wanes. ",ilhJ8BHWE3M
