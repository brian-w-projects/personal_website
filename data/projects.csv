1,Font Color Predictor,09/28/18,https://github.com/brian-w-projects/color_learner,1,Machine learning was used to determine best font color based on background color,This project is designed to decide which of two potential font colors is better suited for reading text against any color background. Machine learning is used to give individualized results for each user. Users answer 100 preferential questions displayed in a tkinter window and this information is used to predict future responses. Font colors may be selected or chosen randomly.,"In designing this project, I made a number of conscious decisions to keep the project as small as possible. Instead of looking for a comprehensive solution to the task, I instead wanted to focus on just how much of the task was achievable with a minimal amount of code. For example, the program generates the background colors randomly instead of systematically presenting a representative sample of the overall color palette. In fact, it is theoretically possible for the same color to be displayed twice (and for the user to pick a different answer each time). Possible extensions to this project could include addressing this issue and allowing for multi-classification instead of just two colors. Despite these purposeful limitations, the algorithm is able to achieve over 90% accuracy on the validation set.",https://youtube.com/embed/D3QkF0KDC2A
2,Hockey Playoff Predictor,10/01/18,https://github.com/brian-w-projects/hockey,1,Machine learning was used to predict which teams will make the playoffs for the 2018-2019 season,This project is designed to predict which teams will make the NHL playoffs based on their statistics halfway through the season. This page will be updated in January to make my predictions for the current season and in April to reflect on how successful the project was. The validation set correctly identified ~90% of target values which corresponds to about 28 or 29 of the 31 teams correctly predicted.,The workflow on this project is purposefully not typical. I specifically prepared the scraped data for storage in MySQL and then wrote the queries to gather the information needed to write the machine learning algorithm. I did this to demonstrate my comfort in working in MySQL.\n\nHistorical data for the past decade was scraped in Python and formatted to be stored properly in MySQL. I then wrote a query to gather all the information required and sent that back to Python to use Scikit-Learn to make my predictions.,https://youtube.com/embed/ykzbCkNoTSo
3,Blockbuster Movie Boxoffice,06/01/18,https://github.com/brian-cuny/607finalproject,1,Scraping and analysis on recent movie blockbusters,This data analysis project was written in R to demonstrate comfort with complex scraping and munging tasks. Pulling data from multiple sources I compiled information on the top 50 blockbuster movies for each of the past 10 years. I then created a Shiny App to explore this data.,"The data for this project was pulled from several sources. I used traditional scraping to gather the names of popular movies and their box office totals. I then used an API to query information from IMDB. Finally, I scraped information from Wikipedia to gather details on the genders of the leads. All of this was cleaned and combined into the final project.\n\n Identifying movie stars genders was the most challenging task as there is no simple resource that contains this information. I decided to use Wikipedia as their standardized format for writing articles made it possible to scrape the first paragraph and analysis it’s text to make a determination.",https://youtube.com/embed/5L5h_gu_ec8
4,My Personal Website,10/04/18,https://github.com/brian-w-projects/personal_website,1,This website you are currently viewing,"I developed this website not only to have a portfolio of my projects but also to demonstrate my comfort working with web technologies. I designed this entire site from scratch using HTML, CSS, Bootstrap and some basic Jquery for the front end and Flask (Python) and SQL on the back end. The site is currently being hosted on Heroku.","Although it is perhaps not common for a data scientist to create web sites, I believe that comfort with these technologies acts as a great added value. For example, presentations in Rmarkdown can be styled with CSS and Bootstrap and being able to get a web app up and running quickly can be helpful for testing purposes. It also provides me a number of opportunities to practice SQL schema design and querying.",none
5,Java API Example,05/30/17,https://github.com/brian-w-projects/access_recommend_me/tree/master/src/recommendation,1,Java program to access personal site API with numerous customization options,"As a beginning project when I was first learning Flask, I developed a site I called “Recommend Me” that essentially acted as a Twitter clone. The project allowed me to focus on the front and back end of web development by recreating a popular website. The final step in this project was learning how to create an API to allow easier access to the site. I decided to test the design of the API by creating this Java project.","This Java project allows for complete interaction with my Twitter clone website via it’s public facing API. Every action on the site can be completed either via the command line or with a text file of commands. The commands also feature a number of convenience wrappers for accessing larger amounts of information with few commands. \n\n Although the program itself is usable only with the “Recommend Me” site, I believe it is representative of the benefit of having knowledge in a variety of programming languages. My knowledge in Java allowed for the creation of this program to test my more newly acquired skills. I believe it is important to have comfort in both higher level and lower level languages as both have their place depending on the core task at hand.",none
6,Health Tweets Data Exploration,10/10/18,https://github.com/brian-w-projects/health_tweets,1,Data exploration and analysis presentation on 60k tweets from health new services,"I set out on this project with no singular goal in mind, instead focusing on using my data exploration and analysis skills to discover whatever I could about the data set. I let the information I discovered lead me to the next discovery and then drew conclusions from everything I had amassed. While it is often the case that a data science sets out to solve a singular problem, I believe it is a beneficial experience to simply explore the data. It can sometimes lead to unexpected truths that were not known before.","I dove into the data set initially by parsing it in R and performing sentiment analysis on the data. I prefer R’s tidytext tools to Pythons and thus began my exploration there. I was able to identify some broad trends between the different types of new organizations from their use of @ symbols, # and sentimental words. This exploration clearly shows the different tactics used by organizations to drive traffic to their site. \n\n I decided to use Python for the unsupervised machine learning portion of the exploration as I prefer Python’s scikit-learn interface. One of the major benefits of being comfortable using both R and Python is that it gives you access to variety of tools so that I can select the one best suited for the task at hand. The unsupervised learning worked better than I could have hoped for. Of the 25 topics created, more than 20 had clear topics and voices. This aided in creating my final conclusions for the project and once again reinforced that while tweets may give the appearance of casual discussion, this is actually not the case at all.",https://www.youtube.com/embed/sV1AMqjZTTw
