1,Font Color Predictor,09/28/18,https://github.com/brian-w-projects/color_learner,1,Machine learning was used to determine best font color based on background color,This project is designed to decide which of two potential font colors is better suited for reading text against any color background. Machine learning is used to give individualized results for each user. Users answer 100 preferential questions displayed in a tkinter window and this information is used to predict future responses. Font colors may be selected or chosen randomly.,"In designing this project, I made a number of conscious decisions to keep the project as small as possible. Instead of looking for a comprehensive solution to the task, I instead wanted to focus on just how much of the task was achievable with a minimal amount of code. For example, the program generates the background colors randomly instead of systematically presenting a representative sample of the overall color palette. In fact, it is theoretically possible for the same color to be displayed twice (and for the user to pick a different answer each time). Possible extensions to this project could include addressing this issue and allowing for multi-classification instead of just two colors. Despite these purposeful limitations, the algorithm is able to achieve over 90% accuracy on the validation set.",https://youtube.com/embed/D3QkF0KDC2A
2,Hockey Playoff Predictor,10/01/18,https://github.com/brian-w-projects/hockey,1,Machine learning was used to predict which teams will make the playoffs for the 2018-2019 season,This project is designed to predict which teams will make the NHL playoffs based on their statistics halfway through the season. This page will be updated in January to make my predictions for the current season and in April to reflect on how successful the project was. The validation set correctly identified ~90% of target values which corresponds to about 28 or 29 of the 31 teams correctly predicted.,The workflow on this project is purposefully not typical. I specifically prepared the scraped data for storage in MySQL and then wrote the queries to gather the information needed to write the machine learning algorithm. I did this to demonstrate my comfort in working in MySQL.\n\nHistorical data for the past decade was scraped in Python and formatted to be stored properly in MySQL. I then wrote a query to gather all the information required and sent that back to Python to use Scikit-Learn to make my predictions.,https://youtube.com/embed/ykzbCkNoTSo
3,Blockbuster Movie Boxoffice,06/01/18,https://github.com/brian-cuny/607finalproject,1,Scraping and analysis on recent movie blockbusters,This data analysis project was written in R to demonstrate comfort with complex scraping and munging tasks. Pulling data from multiple sources I compiled information on the top 50 blockbuster movies for each of the past 10 years. I then created a Shiny App to explore this data.,"The data for this project was pulled from several sources. I used traditional scraping to gather the names of popular movies and their box office totals. I then used an API to query information from IMDB. Finally, I scraped information from Wikipedia to gather details on the genders of the leads. All of this was cleaned and combined into the final project.\n\n Identifying movie stars genders was the most challenging task as there is no simple resource that contains this information. I decided to use Wikipedia as their standardized format for writing articles made it possible to scrape the first paragraph and analysis it’s text to make a determination.",https://youtube.com/embed/5L5h_gu_ec8
4,My Personal Website,10/04/18,https://github.com/brian-w-projects/personal_website,1,This website you are currently viewing,"I developed this website not only to have a portfolio of my projects but also to demonstrate my comfort working with web technologies. I designed this entire site from scratch using HTML, CSS, Bootstrap and some basic Jquery for the front end and Flask (Python) and SQL on the back end. The site is currently being hosted on Heroku.","Although it is perhaps not common for a data scientist to create web sites, I believe that comfort with these technologies acts as a great added value. For example, presentations in Rmarkdown can be styled with CSS and Bootstrap and being able to get a web app up and running quickly can be helpful for testing purposes. It also provides me a number of opportunities to practice SQL schema design and querying.",none
5,Java API Example,05/30/17,https://github.com/brian-w-projects/access_recommend_me/tree/master/src/recommendation,1,Java program to access personal site API with numerous customization options,"As a beginning project when I was first learning Flask, I developed a site I called “Recommend Me” that essentially acted as a Twitter clone. The project allowed me to focus on the front and back end of web development by recreating a popular website. The final step in this project was learning how to create an API to allow easier access to the site. I decided to test the design of the API by creating this Java project.","This Java project allows for complete interaction with my Twitter clone website via it’s public facing API. Every action on the site can be completed either via the command line or with a text file of commands. The commands also feature a number of convenience wrappers for accessing larger amounts of information with few commands. \n\n Although the program itself is usable only with the “Recommend Me” site, I believe it is representative of the benefit of having knowledge in a variety of programming languages. My knowledge in Java allowed for the creation of this program to test my more newly acquired skills. I believe it is important to have comfort in both higher level and lower level languages as both have their place depending on the core task at hand.",none
6,Health Tweets Data Exploration,10/10/18,https://github.com/brian-w-projects/health_tweets,1,Data exploration and analysis presentation on 60k tweets from health new services,"I set out on this project with no singular goal in mind, instead focusing on using my data exploration and analysis skills to discover whatever I could about the data set. I let the information I discovered lead me to the next discovery and then drew conclusions from everything I had amassed. While it is often the case that a data science sets out to solve a singular problem, I believe it is a beneficial experience to simply explore the data. It can sometimes lead to unexpected truths that were not known before.","I dove into the data set initially by parsing it in R and performing sentiment analysis on the data. I prefer R’s tidytext tools to Pythons and thus began my exploration there. I was able to identify some broad trends between the different types of new organizations from their use of @ symbols, # and sentimental words. This exploration clearly shows the different tactics used by organizations to drive traffic to their site. \n\n I decided to use Python for the unsupervised machine learning portion of the exploration as I prefer Python’s scikit-learn interface. One of the major benefits of being comfortable using both R and Python is that it gives you access to variety of tools so that I can select the one best suited for the task at hand. The unsupervised learning worked better than I could have hoped for. Of the 25 topics created, more than 20 had clear topics and voices. This aided in creating my final conclusions for the project and once again reinforced that while tweets may give the appearance of casual discussion, this is actually not the case at all.",https://www.youtube.com/embed/sV1AMqjZTTw
7,Image Recognition Sodoku Solver,11/09/18,https://github.com/brian-cuny/622final,1,Use Image Recognition and Machine Learning to solve imported Sodoku images,"This project serves as a proof of concept for a large scale puzzle solving machine learning pipeline. In it, I use a combination of image recognition and machine learning to solve Sodoku puzzles. Users can submit Sodoku images that are processed and solved. The general outline can be adapted to solve a variety of puzzles.","The Sodoku Solver generally comprises of three separate steps, each of which can be modified or expanded to to encompass a larger problem set. These steps are: \n\n 1. Ingesting a Sodoku puzzle image and transforming it into 81 separate 2D numpy arrays containing the black/white pixel values in each cell. \n 2. Sending the data through a machine learning pipeline to categorize the 10 difference symbols that each cell could contain. \n 3. Ingesting a Sodoku image, predicting the values in each cell and solving the remaining pieces of the puzzle. \n\n The most important challenge of this project was to ensure an overall high accuracy and recall on the ML algorithm. In this case, an accuracy of 90% would not be acceptable as across the approximately 81 predictions for each puzzle even a single mistake would result in an incorrect (and likely unsolvable) Sodoku puzzle. The final implementation of the project achieved it’s high accuracy not from a complex ML pipeline, but instead from careful image ingestion. I needed to box each digit as closely as possible to account for differing sizes and fonts in the images. I discuss some potential related projects in the video.",https://www.youtube.com/embed/VxHhBzMB_4A
8,USA Metro Area Population Visualizations,11/29/18,https://github.com/brian-cuny/population_flask_app,1,US Metro Area Population Changes Plotly visualizations deployed as a web app using Flask + MySQL as a back end,"This project allows for the exploration of population trends throughout the major US metropolitan areas over the past decade. The creation of this project allowed me to use a variety of tool including MySQL, Python, Flask, HTML, jQuery and Plotly. Great care was taken to push as much of the work as possible to the front end, to alleviate any bottle-necking.","This was the final project for my CUNY Data Visualizations course. The project consists of three separate phases, each highlighting a different area of the data science pipe line. \n\n First, I needed to scrape and clean the required data sets. The data was supplied by a variety of US government websites and consisted of the 300+ official US metro area designations, their associated counties and populations for the years from 2010 to 2017. The data was munged and placed into a MySQL database for easy querying. \n\n Second, I created a Flask app to serve the content. While Plotly can be coded in a variety of languages, I wanted the back end to be streamlined in order to serve as many concurrent users as possible. As a result, the app itself has very little actual content outside of accepting requests and serving the associated data from the database. \n\n Finally, the bulk of the project consisted of the single web page that allows users to request information on metro areas and serves them with a series of interactive displays to aid in examining the data. This required using my knowledge of HTML, Bootstrap, jQuery and Plotly. All of the data manipulations and plot generation is performed on the front end. It is my hope that using this page will allow analysis of populations trends in the United States.",https://www.youtube.com/embed/KDrHl8qwD24
9,Hockey Playoff Predictor – Distributed,01/16/19,https://github.com/brian-w-projects/hockey,1,Simulating Big Data processing and Machine Learning on Google Cloud Platform,"This project is a revisit of a the previously listed ‘Hockey Playoff Predictor’ project. I migrated the previous project to Google Cloud Platform and refactored all of my code to function in a distributed fashion. I utilized a number of GCP services including dataproc and Big Query. Throughout the process I simulated working with big data even though the data set is small enough to be worked on locally. \n\n The pipeline I created involves scraping the raw data from the internet, extracting the necessary information and saving it to Big Query. From there the data was further processed until it could be loaded into Spark to create a machine learning model.",I had previously completed a project that scraped past NHL scores and used this information to create a machine learning model to predict whether a team would make the playoffs based on their record half way through the season. That project is also view-able on my website. I left the project with the promise to revisit the project in January to make my predictions. Instead of just running the model against the new test data I decided to challenge myself and move all of the work I had previously completed to the cloud. \n\n Throughout the project I acted as if the data were too large to work with on my computer and thus needed Google Cloud Platform and distributed computing to complete the desired tasks. This allowed me to demonstrate working with big data while keeping my costs for using GCP manageable. I migrated my project to the cloud and compared my predictions between my sci-kit learn and SparkML models and found them to be identical. \n\n I plan to revisit this project again in the future to create a full pipeline to automate future updates.,https://www.youtube.com/embed/HBgS6G8JHFc
10,Pokemon Pokedex Generator,04/01/19,https://github.com/brian-w-projects/pokedex_generator ,1,Using a neural network to create Pokedex entries.,As a fun project for April Fools Day I created a small neural network and trained it on all the existing Pokedex entries from the Pokemon series of games. The trained network can generated new Pokedex entries by rearranging and connecting common words and phrases from existing entries.,"I began by scraping from the internet all the existing Pokedex entries, tokenizing the results and splitting the results into trigrams. I created a neural net based on a LSTM that takes two words as input and learned the third word. This project is significantly different than a traditional deep learning project as it begins as a supervised learning problem but results in a model that develops entirely new results. In order to achieve this, a few modifications needed to be made. \n\n First, I did not want the model to have too much predictive ability. If the model could too accurately predict the “correct” next word then I would simply end up recreating existing Pokedex entries. I needed to find a balance that allowed for variation in the responses. I can further modulate this randomness with a variable that alters the output distribution of word selection and then selecting the next word with a random weighted selection. Feeding this result back into the net over and over creates a variety of Pokedex entries based on one or two seed words. I found this project to be incredibly fun and enjoyable as a side project.",none
